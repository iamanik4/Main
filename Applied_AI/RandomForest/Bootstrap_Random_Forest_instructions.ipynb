{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AjzGopb_YcKR"
   },
   "source": [
    "# Application of Bootstrap samples in Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZSCtDI6YcKT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2Y1Z1DoYcKZ"
   },
   "source": [
    " <li> Load the boston house dataset </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wBWRNKCDYcKb"
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "x=boston.data #independent variables\n",
    "y=boston.target #target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTbK20-mWYHU",
    "outputId": "b473b251-a104-44df-f6c3-3427184c9042"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9690e+02, 9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n",
       "        7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02,\n",
       "        1.7800e+01, 3.9283e+02, 4.0300e+00],\n",
       "       [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n",
       "        6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02,\n",
       "        1.8700e+01, 3.9463e+02, 2.9400e+00]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1,2,3],:]#delete after use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JJ_FwP7xYcKg"
   },
   "source": [
    "### Task: 1\n",
    "<font color='red'><b>Step 1 Creating samples: </b></font> Randomly create 30 samples from the whole boston data points.\n",
    "<ol>\n",
    "<li>Creating each sample: Consider any random 303(60% of 506) data points from whole data set and then replicate any 203 points from the sampled points</li>\n",
    "<li>Ex: For better understanding of this procedure lets check this examples, assume we have 10 data points [1,2,3,4,5,6,7,8,9,10], first we take 6 data points randomly consider we have selected [4, 5, 7, 8, 9, 3] now we will replciate 4 points from [4, 5, 7, 8, 9, 3], consder they are [5, 8, 3,7] so our final sample will be [4, 5, 7, 8, 9, 3, 5, 8, 3,7]</li>\n",
    "<li> we create 30 samples like this </li>\n",
    "<li> Note that as a part of the Bagging when you are taking the random samples make sure each of the sample will have                different set of columns</li>\n",
    "<li> Ex: assume we have 10 columns for the first sample we will select [3, 4, 5, 9, 1, 2] and for the second sample [7, 9, 1, 4, 5, 6, 2] and so on...</li>\n",
    "<li> Make sure each sample will have atleast 3 feautres/columns/attributes</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 2 Building High Variance Models on each of the sample and finding train MSE value:</b></font> Build a DecisionTreeRegressor on each of the sample.\n",
    "<ol><li>Build a regression trees on each of 30 samples.</li>\n",
    "<li>computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li> predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{30}\\sum_{k=1}^{30}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $MSE =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "<font color='red'><b>Step 3 Calculating the OOB score :</b></font>\n",
    "<ol>\n",
    "<li>Computed the predicted values of each data point(506 data points) in your corpus.</li>\n",
    "<li>Predicted house price of $i^{th}$ data point $y^{i}_{pred} =  \\frac{1}{k}\\sum_{\\text{k= model which was buit on samples not included } x^{i}}(\\text{predicted value of } x^{i} \\text{ with } k^{th} \\text{ model})$.</li>\n",
    "<li>Now calculate the $OOB Score =  \\frac{1}{506}\\sum_{i=1}^{506}(y^{i} - y^{i}_{pred})^{2}$.</li>\n",
    "</ol>\n",
    "\n",
    "### Task: 2\n",
    "<pre>\n",
    "<font color='red'><b>Computing CI of OOB Score and Train MSE</b></font>\n",
    "<ol>\n",
    "<li> Repeat Task 1 for 35 times, and for each iteration store the Train MSE and OOB score </li>\n",
    "<li> After this we will have 35 Train MSE values and 35 OOB scores </li>\n",
    "<li> using these 35 values (assume like a sample) find the confidence intravels of MSE and OOB Score </li>\n",
    "<li> you need to report CI of MSE and CI of OOB Score </li>\n",
    "<li> Note: Refer the Central_Limit_theorem.ipynb to check how to find the confidence intravel</li>\n",
    "</ol>\n",
    "</pre>\n",
    "### Task: 3\n",
    "<pre>\n",
    "<font color='red'><b>Given a single query point predict the price of house.</b></font>\n",
    "\n",
    "<li>Consider xq= [0.18,20.0,5.00,0.0,0.421,5.60,72.2,7.95,7.0,30.0,19.1,372.13,18.60] Predict the house price for this point as mentioned in the step 2 of Task 1. </li>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n",
      "(288,)\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "row_indices_iob = []\n",
    "row_indices_oob = []\n",
    "col_indices     = []\n",
    "\n",
    "for i in range(30):\n",
    "    iob_indices = np.random.choice(range(506),303)\n",
    "    iob_indices = np.hstack([iob_indices,np.random.choice(iob_indices,size=203)])\n",
    "    col_index = np.array(random.sample(range(13),random.randint(3,13)))\n",
    "    row_indices_iob.append(iob_indices) \n",
    "    col_indices.append(col_index)\n",
    "    oob_indices = np.setdiff1d(range(506),iob_indices)\n",
    "    row_indices_oob.append(oob_indices)\n",
    "print(row_indices_iob[29].shape)\n",
    "print(row_indices_oob[29].shape)\n",
    "print(col_indices[29].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.030107150427898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "Y_pred = [0 for i in range(506)]\n",
    "for i in range(30):\n",
    "    clf = DecisionTreeRegressor()\n",
    "    X_train = x[row_indices_iob[i][:,None],col_indices[i]]\n",
    "    Y_train = y[row_indices_iob[i]]\n",
    "    clf.fit(X_train,Y_train)\n",
    "    Y_pred += clf.predict(x[:,col_indices[i]])\n",
    "Y_pred = Y_pred/30\n",
    "\n",
    "errors = (y-Y_pred)**2\n",
    "MSE = errors.sum()/506\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.432277641526976\n"
     ]
    }
   ],
   "source": [
    "Y_pred_OOB = [0 for i in range(506)]\n",
    "Ks = [0 for i in range(506)]\n",
    "\n",
    "for i in range(30):\n",
    "    k = 0\n",
    "    clf = DecisionTreeRegressor()\n",
    "    X_train = x[row_indices_iob[i][:,None],col_indices[i]]\n",
    "    Y_train = y[row_indices_iob[i]]\n",
    "    clf.fit(X_train,Y_train)\n",
    "\n",
    "    for j in range(506):\n",
    "        if j not in row_indices_iob[i]:\n",
    "            Ks[j] += 1\n",
    "            Y_pred_OOB[k] += clf.predict(x[j,col_indices[i]].reshape(1,-1)).item()\n",
    "        k += 1\n",
    "        \n",
    "Y_pred_OOB = [x/y for x,y in zip(Y_pred_OOB,Ks)]\n",
    "\n",
    "errors_OOB = (y-Y_pred_OOB)**2\n",
    "OOB = errors_OOB.sum()/506\n",
    "print(OOB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs = []\n",
    "OOBs = []\n",
    "for _ in range(35):\n",
    "    row_indices_iob = []\n",
    "    row_indices_oob = []\n",
    "    col_indices     = []\n",
    "\n",
    "    for i in range(30):\n",
    "        iob_indices = np.random.choice(range(506),303)\n",
    "        iob_indices = np.hstack([iob_indices,np.random.choice(iob_indices,size=203)])\n",
    "        col_index = np.array(random.sample(range(13),random.randint(3,13)))\n",
    "        row_indices_iob.append(iob_indices) \n",
    "        col_indices.append(col_index)\n",
    "        oob_indices = np.setdiff1d(range(506),iob_indices)\n",
    "        row_indices_oob.append(oob_indices)\n",
    "    \n",
    "    Y_pred = [0 for i in range(506)]\n",
    "    for i in range(30):\n",
    "        clf = DecisionTreeRegressor()\n",
    "        X_train = x[row_indices_iob[i][:,None],col_indices[i]]\n",
    "        Y_train = y[row_indices_iob[i]]\n",
    "        clf.fit(X_train,Y_train)\n",
    "        Y_pred += clf.predict(x[:,col_indices[i]])\n",
    "    Y_pred = Y_pred/30\n",
    "\n",
    "    errors = (y-Y_pred)**2\n",
    "    MSE = errors.sum()/506\n",
    "    MSEs.append(MSE)\n",
    "    \n",
    "    Y_pred_OOB = [0 for i in range(506)]\n",
    "    Ks = [0 for i in range(506)]\n",
    "\n",
    "    for i in range(30):\n",
    "        k = 0\n",
    "        clf = DecisionTreeRegressor()\n",
    "        X_train = x[row_indices_iob[i][:,None],col_indices[i]]\n",
    "        Y_train = y[row_indices_iob[i]]\n",
    "        clf.fit(X_train,Y_train)\n",
    "\n",
    "        for j in range(506):\n",
    "            if j not in row_indices_iob[i]:\n",
    "                Ks[j] += 1\n",
    "                Y_pred_OOB[k] += clf.predict(x[j,col_indices[i]].reshape(1,-1)).item()\n",
    "            k += 1\n",
    "\n",
    "    Y_pred_OOB = [x/y for x,y in zip(Y_pred_OOB,Ks)]\n",
    "\n",
    "    errors_OOB = (y-Y_pred_OOB)**2\n",
    "    OOB = errors_OOB.sum()/506\n",
    "    OOBs.append(OOB)\n",
    "    \n",
    "print(MSEs)\n",
    "print(OOBs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bootstrap_Random_Forest_instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
