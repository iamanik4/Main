{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 9E and 9F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 9E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 0, ..., 1, 0, 1])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y[y==0] = -1\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(3200, 5) (3200,)\n(800, 5) (800,)\n(1000, 5) (1000,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=0.2,random_state=24)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can write your code here\n",
    "gamma = 0.001\n",
    "clf = SVC(gamma=gamma, C=100)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K(xq):\n",
    "    val = 0\n",
    "    for alpha,xi in zip(clf.dual_coef_[0],clf.support_vectors_): #the dual_coef_[i] contains label[i]*alpha[i]\n",
    "        val += alpha*np.exp(-gamma*np.linalg.norm(xi-xq)**2) \n",
    "    return val+clf.intercept_.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_fun(X_val):\n",
    "    fcv = []\n",
    "    for xq in X_val:\n",
    "        fcv.append(K(xq))\n",
    "    return(np.array(fcv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-2.69065026, -4.01123357, -2.48966713,  1.35046624, -2.59528514,\n       -2.30501509,  1.41770579, -0.43831055, -2.43467245,  1.64358234,\n       -2.30755842, -1.28262667,  1.43221735, -3.6640798 ,  1.73676085,\n        1.64844048, -2.96446415, -2.27370879, -3.87103916, -1.38041339,\n       -2.2897439 ,  1.35763225, -1.58450812, -2.71269654, -2.51785957,\n        1.59448568, -2.91446027, -1.14699993, -1.27549829,  1.7822262 ,\n       -3.69602334, -2.42593879,  1.89868102, -0.08398933,  1.66267593,\n        1.4698012 , -3.25798546, -3.45051326, -0.09489246,  0.09058346,\n       -2.02851987,  1.1774024 , -2.91011102, -2.6490935 , -0.40433583,\n        2.08016708, -3.4494807 , -1.04782432,  0.58895649, -2.34283292,\n       -3.98910774, -0.49050617, -3.57258524, -1.28506826, -1.3270522 ,\n       -2.62622674, -2.85807125, -2.08467843, -2.3924406 ,  2.01477986,\n       -2.63538773,  0.94164139, -1.16507049,  0.23152835,  2.95486314,\n       -0.14166578, -3.1806057 , -2.06875309,  2.14263081,  1.76054525,\n       -2.48781106,  1.57899689, -0.1355476 ,  0.21906615, -0.19229226,\n       -3.08421123, -0.26833131,  1.24499086, -2.73909828, -0.65983288,\n       -4.40626063, -1.79474729, -4.30854327,  2.51693797, -0.16540411,\n       -4.29419235, -2.79777362, -0.01309788, -3.02891955, -2.16001371,\n        0.35140248, -1.60224582, -0.26803931, -1.53626454, -2.05392911,\n        1.69552827, -2.39475142, -3.49774855, -0.98200507,  1.54840541,\n       -0.79460672, -0.19597257, -2.16372727, -3.56325034,  2.07600895,\n       -1.78182574, -3.09877972, -1.64571368, -2.15383029,  2.30892744,\n        0.12854562, -2.35317516, -6.48193871,  1.64480214,  0.06223518,\n       -3.63075923, -4.05448655, -2.83629348, -3.28919081, -0.64461743,\n       -1.64883157,  0.94492196,  0.28759072, -2.45598367, -2.15073026,\n       -1.28350098, -3.07593075, -1.31960497, -1.68054445, -2.40760088,\n        1.6151082 ,  1.52919191,  1.97478295, -2.96447607, -2.2424657 ,\n        1.94061512,  1.29985123, -0.51701875, -3.84336513, -3.80537519,\n       -2.25279749,  1.11898727, -2.44824548, -1.73258569, -2.45588294,\n       -2.82003191, -2.11741714,  1.49637453, -1.94309506, -3.17828529,\n       -2.48727134, -2.49029132, -2.8380681 , -2.21660455,  0.21366241,\n       -2.76828728,  1.17298869,  2.18970905,  1.38464782, -2.92718541,\n        0.07178942,  0.42080825,  1.5071863 , -0.94128895,  1.95133503,\n        2.78977065, -3.22032733, -2.5130945 , -2.81045955,  0.83006113,\n       -1.58746749, -2.81713355,  1.00676107, -1.70882128, -4.50603018,\n        1.87142595, -1.80633552, -4.12530367, -2.97532286,  1.31560915,\n        2.79898904, -3.49403293, -2.62168611,  0.50689727,  1.56573518,\n       -1.95078375,  1.29389626, -4.03752244,  0.42143692, -2.61853706,\n       -3.27061281,  4.19002027,  1.98722705, -4.19009109,  1.5997084 ,\n        1.80916733,  2.22301146, -2.07025532, -1.23100049, -2.17820903,\n       -3.20251597, -3.3568973 , -4.469556  , -2.86005588, -2.40761792,\n       -3.94935817, -4.7700457 , -2.47985146, -3.99426899,  2.53611521,\n        1.83822497,  1.45681349, -2.14003322, -1.00045143, -3.99824316,\n        2.04411787,  1.39455864,  1.45768061, -0.7456149 ,  0.9898566 ,\n        0.94072193, -2.90582647, -2.54227704,  1.86272233, -1.2811299 ,\n       -1.0729985 , -1.81537833,  1.76071954, -2.31089262,  1.93200482,\n       -3.00788281,  1.60282868, -0.35042226, -3.04138086, -2.98154489,\n       -2.1339324 , -1.2892673 , -3.16736992, -2.79265153, -1.60140587,\n        0.39732518,  1.38290215,  2.31604749,  2.14610542,  1.42845912,\n       -3.75750388,  1.61067114, -1.70500309, -2.327318  , -2.80867803,\n       -1.3607949 ,  1.76091421,  0.70822991, -2.63240498, -1.97138434,\n        1.28041482, -3.70714305, -2.82499848, -2.29286001, -1.40238527,\n       -0.72023564,  0.01175639,  1.50853642, -3.54227214, -0.68157417,\n        0.27943936, -2.75443722,  1.34945654, -3.5325842 , -0.09672848,\n        2.20620486, -3.46320287, -2.25018142, -2.39507978, -3.12144451,\n        1.9247384 , -2.61194182, -2.03520301, -2.59480406, -1.61362593,\n        0.89276729, -3.42258953, -1.2473559 , -3.21777441, -3.41779488,\n       -2.59526315, -2.43215553, -0.65839084,  1.50047744, -2.89384689,\n       -0.27954482,  1.66359224,  1.28061714, -0.09475103, -0.4753708 ,\n       -0.77298959, -2.76228151,  1.66346094, -3.76646662,  0.68285796,\n       -3.0281968 ,  1.73244727,  2.63757108, -1.52995073, -3.95801896,\n        2.55664982, -3.47487022, -4.92061629, -1.90652075,  2.16102975,\n        0.51491322, -2.68971447, -0.58475072, -2.95177726, -3.83387667,\n       -1.90636953, -3.3932233 ,  1.8569623 , -3.55180067, -3.17102837,\n       -4.34843298, -3.21379054,  1.51870657, -1.94362482,  2.29155257,\n        0.44415108,  1.83944899, -2.71327878, -1.36779473, -3.09775044,\n        1.3873724 , -1.82508214,  1.75958189,  1.61437862, -2.56100875,\n        1.62640647, -0.98579393,  2.57261983, -3.05080239, -2.33086585,\n       -1.66148579, -0.56841199, -1.42538559,  0.14199932, -3.29559731,\n        0.04687486, -3.12763601, -3.96497191, -2.8964183 ,  1.89983657,\n       -2.96943496, -0.0404273 , -3.91312035, -2.64288399, -2.24864336,\n        0.89188227,  0.64470293,  0.75701186, -2.84436345, -2.12441753,\n       -1.2448525 ,  1.21945864, -0.99002248, -0.67511869, -0.25991606,\n        2.46587995, -3.38146435,  0.34399523, -3.21203639, -0.77805933,\n       -4.13519945, -0.94837384, -2.88543805, -2.28467542, -1.02571322,\n       -2.46077259, -3.40096428, -2.72102414,  0.59715392, -0.38925944,\n       -2.56112894, -0.67055882, -2.60138593, -0.09935201, -2.46825142,\n        0.16674346, -3.77855416, -3.66074356,  0.63179569, -3.3291185 ,\n       -3.55009003,  1.29247736, -0.7480208 , -0.566622  , -1.9994833 ,\n       -4.1180475 , -2.87602507, -3.21426055,  0.71888701, -1.94688744,\n       -3.15197966,  1.05170195,  1.87496803, -4.03006436,  2.26015192,\n       -1.7321023 , -1.10497414, -1.67858315,  1.84398091,  0.8336729 ,\n       -2.25754102,  1.36227741, -2.28435601,  1.42133086, -2.00971743,\n       -1.75557395, -3.00235279, -0.29906234, -0.30651444, -3.29563215,\n       -3.2134764 , -2.20857426, -2.73011899,  1.37403917, -2.58844476,\n        1.46102594, -1.19466523, -0.71213086, -3.76035193, -4.25835061,\n        2.81791993,  1.82402233, -1.45869585, -0.75208123, -4.52592367,\n       -2.87689496, -2.27912259,  1.1812949 ,  1.67092947, -2.71537558,\n       -2.15667182,  0.24914642,  0.0985699 , -0.68086289, -2.57606839,\n        2.35314009, -2.55808362,  0.81398645, -4.06327393, -2.01825946,\n       -2.44639863, -1.45394311, -1.37798017, -2.45506092, -3.13040792,\n       -2.68233359,  2.10192122, -2.84996151,  1.25948697,  0.66456882,\n       -3.30541189, -2.48997981,  1.7483927 , -2.30839143,  0.15658406,\n       -2.47481232,  1.36236408, -1.92045643, -0.36533402, -2.50433087,\n        0.82988235, -3.67725195, -2.05479977,  0.37134724,  0.52217669,\n        1.02062934,  1.12342676, -2.41099169, -3.18412711, -2.14175607,\n       -0.32885814,  0.29119964, -2.88747485, -2.8321305 , -2.23222501,\n       -2.60604428, -6.79265103,  0.93185395, -0.61777776, -0.64372293,\n        0.01718008,  2.14271767,  1.78074528,  2.02410592, -2.73436333,\n        0.31817742, -2.40130274, -0.53571486, -2.51203706,  1.27075473,\n        2.7883653 , -0.54143484, -2.87902832, -1.98113835, -3.51952828,\n       -3.43290684, -2.69786392, -2.83092411,  1.95491188,  0.34662513,\n       -2.36036033,  1.29609233, -1.245734  ,  1.65089137, -1.73139598,\n       -3.61632894,  1.7205206 , -4.29728983, -1.66798772, -2.94269367,\n       -3.62703064, -0.79722771, -2.59674846, -2.92818312, -1.13351962,\n       -1.6606401 , -2.77162075, -1.46759649, -2.15159046, -4.35572845,\n       -3.44268471, -3.13876084, -3.64225817, -0.07850596, -2.64827379,\n       -2.40605508, -3.62396204, -3.12038012, -0.87698838, -1.40700995,\n       -2.25435365, -3.62572937,  1.27735594,  1.09545222,  2.41998918,\n       -1.25412878, -0.42534663, -3.04536514, -2.32443158, -5.24224622,\n       -2.494916  , -2.90081423, -2.59249069, -0.49733581, -3.14030722,\n       -1.22072991, -3.34479226, -2.34662703,  1.63330426, -3.1598157 ,\n       -2.84376136, -3.26254112, -0.65280941,  1.38772383, -2.5976787 ,\n       -1.79830647,  1.17538855, -1.64359583, -3.32910306, -2.69778067,\n       -3.55407597, -2.50513446,  2.22231121, -3.65641954, -2.15047709,\n        1.6762429 , -3.06720269, -1.50160277,  0.50067103, -1.73192212,\n       -1.46265687, -2.58492586, -3.28011574,  1.54675066, -0.58155581,\n        1.85209793, -1.55220976,  1.73322734, -2.24392856, -2.16258551,\n       -1.79833602,  1.59766791, -1.93528411, -0.72642394, -1.73220157,\n       -2.57870138, -0.88220461, -1.84240123, -2.01819499,  1.9290912 ,\n       -2.0671061 ,  1.77036325,  1.28604682, -2.39433418, -3.78242434,\n       -1.96181254,  1.73353595, -2.26088792, -0.54743446, -2.68474676,\n       -2.90005866, -2.40094625, -2.97467183,  2.52759231,  1.59505975,\n       -0.51860933, -4.02797209,  3.49195209, -1.2457772 , -2.69764283,\n       -0.52549419, -2.67472029, -2.93726613, -3.65547808,  2.2979195 ,\n       -1.32470809, -2.89703532,  0.8693855 , -0.44086201,  1.77895844,\n        1.39321366, -2.32595067, -2.15854099, -2.00252809, -3.69237506,\n       -3.26767904, -2.87650604, -1.94262691, -3.2602958 , -2.2947999 ,\n       -3.5295951 ,  1.68506527, -3.31664806,  1.54329054, -1.67096811,\n       -1.60355335, -1.47613208,  1.0197002 , -2.35790046, -1.99012662,\n        2.33311059, -2.65780187, -2.96377362, -0.53057425, -0.83482634,\n        0.08471591, -2.84726638,  0.93380161, -0.35001737,  1.5907361 ,\n        0.75076547, -2.79261386, -0.43394381,  0.35004408, -3.14080836,\n       -5.22047164, -3.83460404, -0.4023501 , -2.40041692, -1.90983058,\n       -1.57849293, -2.74292745, -3.44334578, -3.83001241, -2.81643129,\n        1.40497589, -2.09554482,  1.75211849, -2.7724323 ,  1.41032381,\n       -0.38241517,  0.41934655,  1.51190612,  0.95267747, -3.1454303 ,\n       -2.82568854, -2.40955041, -2.72771179,  1.74785713, -3.39870585,\n       -1.16976821, -2.49125625, -3.47379241, -2.49450909, -2.40908156,\n       -1.57578501, -2.77335009, -2.55468221, -2.93865345,  1.47231477,\n       -2.21392298, -0.13320623, -3.33838351, -2.49395437, -1.46187321,\n       -2.45160297,  1.24921446, -0.68863506, -1.9932513 , -3.94268154,\n       -3.12748177, -3.34426711,  1.50323602,  0.88773265,  0.18953072,\n        0.98854065, -1.88509827, -0.46199475,  1.23371473,  1.99247125,\n        0.94677159, -1.38059199, -2.95177379, -3.62265413, -4.16259113,\n       -2.90874639, -2.69420517, -1.37910306, -3.13444097, -2.40304626,\n        1.97520482, -0.04576525, -1.40797166, -3.5061022 , -0.20949395,\n       -2.2566609 , -1.62471253,  1.7241465 ,  2.37953104, -2.81010883,\n        2.51245516, -1.34321661,  1.92144992, -1.26507901, -2.76153865,\n        1.39999208, -2.08645107, -3.08384688, -1.53924807,  1.53666113,\n       -1.68808026, -2.45165151, -3.80680579,  1.62159694, -2.90856666,\n       -2.13783026, -3.02717188, -2.26983414, -2.03801713,  0.34467582,\n       -1.56172108, -1.90614552, -4.52000795,  1.49732833, -1.62881311,\n       -2.09937213, -3.43348123,  0.61055333, -2.36975595, -2.89286885,\n       -1.32010793, -2.35191007, -1.42909112, -4.07253748, -0.72015271,\n       -2.481444  ,  1.5762245 , -3.51537142, -2.23132943, -2.83291004,\n       -1.94420318, -2.74069974, -1.51797807, -3.74590304, -0.01594143,\n       -1.84196304, -0.71498972, -2.38548328,  1.08479478,  1.6635698 ,\n       -3.71274778, -3.98625964,  1.36125055, -2.62417272, -3.54212726,\n        1.35868499, -3.62745693, -0.61711105, -1.43596737, -0.85383101])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_fun(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([-2.69065026, -4.01123357, -2.48966713,  1.35046624, -2.59528514,\n       -2.30501509,  1.41770579, -0.43831055, -2.43467245,  1.64358234,\n       -2.30755842, -1.28262667,  1.43221735, -3.6640798 ,  1.73676085,\n        1.64844048, -2.96446415, -2.27370879, -3.87103916, -1.38041339,\n       -2.2897439 ,  1.35763225, -1.58450812, -2.71269654, -2.51785957,\n        1.59448568, -2.91446027, -1.14699993, -1.27549829,  1.7822262 ,\n       -3.69602334, -2.42593879,  1.89868102, -0.08398933,  1.66267593,\n        1.4698012 , -3.25798546, -3.45051326, -0.09489246,  0.09058346,\n       -2.02851987,  1.1774024 , -2.91011102, -2.6490935 , -0.40433583,\n        2.08016708, -3.4494807 , -1.04782432,  0.58895649, -2.34283292,\n       -3.98910774, -0.49050617, -3.57258524, -1.28506826, -1.3270522 ,\n       -2.62622674, -2.85807125, -2.08467843, -2.3924406 ,  2.01477986,\n       -2.63538773,  0.94164139, -1.16507049,  0.23152835,  2.95486314,\n       -0.14166578, -3.1806057 , -2.06875309,  2.14263081,  1.76054525,\n       -2.48781106,  1.57899689, -0.1355476 ,  0.21906615, -0.19229226,\n       -3.08421123, -0.26833131,  1.24499086, -2.73909828, -0.65983288,\n       -4.40626063, -1.79474729, -4.30854327,  2.51693797, -0.16540411,\n       -4.29419235, -2.79777362, -0.01309788, -3.02891955, -2.16001371,\n        0.35140248, -1.60224582, -0.26803931, -1.53626454, -2.05392911,\n        1.69552827, -2.39475142, -3.49774855, -0.98200507,  1.54840541,\n       -0.79460672, -0.19597257, -2.16372727, -3.56325034,  2.07600895,\n       -1.78182574, -3.09877972, -1.64571368, -2.15383029,  2.30892744,\n        0.12854562, -2.35317516, -6.48193871,  1.64480214,  0.06223518,\n       -3.63075923, -4.05448655, -2.83629348, -3.28919081, -0.64461743,\n       -1.64883157,  0.94492196,  0.28759072, -2.45598367, -2.15073026,\n       -1.28350098, -3.07593075, -1.31960497, -1.68054445, -2.40760088,\n        1.6151082 ,  1.52919191,  1.97478295, -2.96447607, -2.2424657 ,\n        1.94061512,  1.29985123, -0.51701875, -3.84336513, -3.80537519,\n       -2.25279749,  1.11898727, -2.44824548, -1.73258569, -2.45588294,\n       -2.82003191, -2.11741714,  1.49637453, -1.94309506, -3.17828529,\n       -2.48727134, -2.49029132, -2.8380681 , -2.21660455,  0.21366241,\n       -2.76828728,  1.17298869,  2.18970905,  1.38464782, -2.92718541,\n        0.07178942,  0.42080825,  1.5071863 , -0.94128895,  1.95133503,\n        2.78977065, -3.22032733, -2.5130945 , -2.81045955,  0.83006113,\n       -1.58746749, -2.81713355,  1.00676107, -1.70882128, -4.50603018,\n        1.87142595, -1.80633552, -4.12530367, -2.97532286,  1.31560915,\n        2.79898904, -3.49403293, -2.62168611,  0.50689727,  1.56573518,\n       -1.95078375,  1.29389626, -4.03752244,  0.42143692, -2.61853706,\n       -3.27061281,  4.19002027,  1.98722705, -4.19009109,  1.5997084 ,\n        1.80916733,  2.22301146, -2.07025532, -1.23100049, -2.17820903,\n       -3.20251597, -3.3568973 , -4.469556  , -2.86005588, -2.40761792,\n       -3.94935817, -4.7700457 , -2.47985146, -3.99426899,  2.53611521,\n        1.83822497,  1.45681349, -2.14003322, -1.00045143, -3.99824316,\n        2.04411787,  1.39455864,  1.45768061, -0.7456149 ,  0.9898566 ,\n        0.94072193, -2.90582647, -2.54227704,  1.86272233, -1.2811299 ,\n       -1.0729985 , -1.81537833,  1.76071954, -2.31089262,  1.93200482,\n       -3.00788281,  1.60282868, -0.35042226, -3.04138086, -2.98154489,\n       -2.1339324 , -1.2892673 , -3.16736992, -2.79265153, -1.60140587,\n        0.39732518,  1.38290215,  2.31604749,  2.14610542,  1.42845912,\n       -3.75750388,  1.61067114, -1.70500309, -2.327318  , -2.80867803,\n       -1.3607949 ,  1.76091421,  0.70822991, -2.63240498, -1.97138434,\n        1.28041482, -3.70714305, -2.82499848, -2.29286001, -1.40238527,\n       -0.72023564,  0.01175639,  1.50853642, -3.54227214, -0.68157417,\n        0.27943936, -2.75443722,  1.34945654, -3.5325842 , -0.09672848,\n        2.20620486, -3.46320287, -2.25018142, -2.39507978, -3.12144451,\n        1.9247384 , -2.61194182, -2.03520301, -2.59480406, -1.61362593,\n        0.89276729, -3.42258953, -1.2473559 , -3.21777441, -3.41779488,\n       -2.59526315, -2.43215553, -0.65839084,  1.50047744, -2.89384689,\n       -0.27954482,  1.66359224,  1.28061714, -0.09475103, -0.4753708 ,\n       -0.77298959, -2.76228151,  1.66346094, -3.76646662,  0.68285796,\n       -3.0281968 ,  1.73244727,  2.63757108, -1.52995073, -3.95801896,\n        2.55664982, -3.47487022, -4.92061629, -1.90652075,  2.16102975,\n        0.51491322, -2.68971447, -0.58475072, -2.95177726, -3.83387667,\n       -1.90636953, -3.3932233 ,  1.8569623 , -3.55180067, -3.17102837,\n       -4.34843298, -3.21379054,  1.51870657, -1.94362482,  2.29155257,\n        0.44415108,  1.83944899, -2.71327878, -1.36779473, -3.09775044,\n        1.3873724 , -1.82508214,  1.75958189,  1.61437862, -2.56100875,\n        1.62640647, -0.98579393,  2.57261983, -3.05080239, -2.33086585,\n       -1.66148579, -0.56841199, -1.42538559,  0.14199932, -3.29559731,\n        0.04687486, -3.12763601, -3.96497191, -2.8964183 ,  1.89983657,\n       -2.96943496, -0.0404273 , -3.91312035, -2.64288399, -2.24864336,\n        0.89188227,  0.64470293,  0.75701186, -2.84436345, -2.12441753,\n       -1.2448525 ,  1.21945864, -0.99002248, -0.67511869, -0.25991606,\n        2.46587995, -3.38146435,  0.34399523, -3.21203639, -0.77805933,\n       -4.13519945, -0.94837384, -2.88543805, -2.28467542, -1.02571322,\n       -2.46077259, -3.40096428, -2.72102414,  0.59715392, -0.38925944,\n       -2.56112894, -0.67055882, -2.60138593, -0.09935201, -2.46825142,\n        0.16674346, -3.77855416, -3.66074356,  0.63179569, -3.3291185 ,\n       -3.55009003,  1.29247736, -0.7480208 , -0.566622  , -1.9994833 ,\n       -4.1180475 , -2.87602507, -3.21426055,  0.71888701, -1.94688744,\n       -3.15197966,  1.05170195,  1.87496803, -4.03006436,  2.26015192,\n       -1.7321023 , -1.10497414, -1.67858315,  1.84398091,  0.8336729 ,\n       -2.25754102,  1.36227741, -2.28435601,  1.42133086, -2.00971743,\n       -1.75557395, -3.00235279, -0.29906234, -0.30651444, -3.29563215,\n       -3.2134764 , -2.20857426, -2.73011899,  1.37403917, -2.58844476,\n        1.46102594, -1.19466523, -0.71213086, -3.76035193, -4.25835061,\n        2.81791993,  1.82402233, -1.45869585, -0.75208123, -4.52592367,\n       -2.87689496, -2.27912259,  1.1812949 ,  1.67092947, -2.71537558,\n       -2.15667182,  0.24914642,  0.0985699 , -0.68086289, -2.57606839,\n        2.35314009, -2.55808362,  0.81398645, -4.06327393, -2.01825946,\n       -2.44639863, -1.45394311, -1.37798017, -2.45506092, -3.13040792,\n       -2.68233359,  2.10192122, -2.84996151,  1.25948697,  0.66456882,\n       -3.30541189, -2.48997981,  1.7483927 , -2.30839143,  0.15658406,\n       -2.47481232,  1.36236408, -1.92045643, -0.36533402, -2.50433087,\n        0.82988235, -3.67725195, -2.05479977,  0.37134724,  0.52217669,\n        1.02062934,  1.12342676, -2.41099169, -3.18412711, -2.14175607,\n       -0.32885814,  0.29119964, -2.88747485, -2.8321305 , -2.23222501,\n       -2.60604428, -6.79265103,  0.93185395, -0.61777776, -0.64372293,\n        0.01718008,  2.14271767,  1.78074528,  2.02410592, -2.73436333,\n        0.31817742, -2.40130274, -0.53571486, -2.51203706,  1.27075473,\n        2.7883653 , -0.54143484, -2.87902832, -1.98113835, -3.51952828,\n       -3.43290684, -2.69786392, -2.83092411,  1.95491188,  0.34662513,\n       -2.36036033,  1.29609233, -1.245734  ,  1.65089137, -1.73139598,\n       -3.61632894,  1.7205206 , -4.29728983, -1.66798772, -2.94269367,\n       -3.62703064, -0.79722771, -2.59674846, -2.92818312, -1.13351962,\n       -1.6606401 , -2.77162075, -1.46759649, -2.15159046, -4.35572845,\n       -3.44268471, -3.13876084, -3.64225817, -0.07850596, -2.64827379,\n       -2.40605508, -3.62396204, -3.12038012, -0.87698838, -1.40700995,\n       -2.25435365, -3.62572937,  1.27735594,  1.09545222,  2.41998918,\n       -1.25412878, -0.42534663, -3.04536514, -2.32443158, -5.24224622,\n       -2.494916  , -2.90081423, -2.59249069, -0.49733581, -3.14030722,\n       -1.22072991, -3.34479226, -2.34662703,  1.63330426, -3.1598157 ,\n       -2.84376136, -3.26254112, -0.65280941,  1.38772383, -2.5976787 ,\n       -1.79830647,  1.17538855, -1.64359583, -3.32910306, -2.69778067,\n       -3.55407597, -2.50513446,  2.22231121, -3.65641954, -2.15047709,\n        1.6762429 , -3.06720269, -1.50160277,  0.50067103, -1.73192212,\n       -1.46265687, -2.58492586, -3.28011574,  1.54675066, -0.58155581,\n        1.85209793, -1.55220976,  1.73322734, -2.24392856, -2.16258551,\n       -1.79833602,  1.59766791, -1.93528411, -0.72642394, -1.73220157,\n       -2.57870138, -0.88220461, -1.84240123, -2.01819499,  1.9290912 ,\n       -2.0671061 ,  1.77036325,  1.28604682, -2.39433418, -3.78242434,\n       -1.96181254,  1.73353595, -2.26088792, -0.54743446, -2.68474676,\n       -2.90005866, -2.40094625, -2.97467183,  2.52759231,  1.59505975,\n       -0.51860933, -4.02797209,  3.49195209, -1.2457772 , -2.69764283,\n       -0.52549419, -2.67472029, -2.93726613, -3.65547808,  2.2979195 ,\n       -1.32470809, -2.89703532,  0.8693855 , -0.44086201,  1.77895844,\n        1.39321366, -2.32595067, -2.15854099, -2.00252809, -3.69237506,\n       -3.26767904, -2.87650604, -1.94262691, -3.2602958 , -2.2947999 ,\n       -3.5295951 ,  1.68506527, -3.31664806,  1.54329054, -1.67096811,\n       -1.60355335, -1.47613208,  1.0197002 , -2.35790046, -1.99012662,\n        2.33311059, -2.65780187, -2.96377362, -0.53057425, -0.83482634,\n        0.08471591, -2.84726638,  0.93380161, -0.35001737,  1.5907361 ,\n        0.75076547, -2.79261386, -0.43394381,  0.35004408, -3.14080836,\n       -5.22047164, -3.83460404, -0.4023501 , -2.40041692, -1.90983058,\n       -1.57849293, -2.74292745, -3.44334578, -3.83001241, -2.81643129,\n        1.40497589, -2.09554482,  1.75211849, -2.7724323 ,  1.41032381,\n       -0.38241517,  0.41934655,  1.51190612,  0.95267747, -3.1454303 ,\n       -2.82568854, -2.40955041, -2.72771179,  1.74785713, -3.39870585,\n       -1.16976821, -2.49125625, -3.47379241, -2.49450909, -2.40908156,\n       -1.57578501, -2.77335009, -2.55468221, -2.93865345,  1.47231477,\n       -2.21392298, -0.13320623, -3.33838351, -2.49395437, -1.46187321,\n       -2.45160297,  1.24921446, -0.68863506, -1.9932513 , -3.94268154,\n       -3.12748177, -3.34426711,  1.50323602,  0.88773265,  0.18953072,\n        0.98854065, -1.88509827, -0.46199475,  1.23371473,  1.99247125,\n        0.94677159, -1.38059199, -2.95177379, -3.62265413, -4.16259113,\n       -2.90874639, -2.69420517, -1.37910306, -3.13444097, -2.40304626,\n        1.97520482, -0.04576525, -1.40797166, -3.5061022 , -0.20949395,\n       -2.2566609 , -1.62471253,  1.7241465 ,  2.37953104, -2.81010883,\n        2.51245516, -1.34321661,  1.92144992, -1.26507901, -2.76153865,\n        1.39999208, -2.08645107, -3.08384688, -1.53924807,  1.53666113,\n       -1.68808026, -2.45165151, -3.80680579,  1.62159694, -2.90856666,\n       -2.13783026, -3.02717188, -2.26983414, -2.03801713,  0.34467582,\n       -1.56172108, -1.90614552, -4.52000795,  1.49732833, -1.62881311,\n       -2.09937213, -3.43348123,  0.61055333, -2.36975595, -2.89286885,\n       -1.32010793, -2.35191007, -1.42909112, -4.07253748, -0.72015271,\n       -2.481444  ,  1.5762245 , -3.51537142, -2.23132943, -2.83291004,\n       -1.94420318, -2.74069974, -1.51797807, -3.74590304, -0.01594143,\n       -1.84196304, -0.71498972, -2.38548328,  1.08479478,  1.6635698 ,\n       -3.71274778, -3.98625964,  1.36125055, -2.62417272, -3.54212726,\n        1.35868499, -3.62745693, -0.61711105, -1.43596737, -0.85383101])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 9F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0. 1.] [550 250]\n"
    }
   ],
   "source": [
    "fcv = dec_fun(X_val)\n",
    "y_val_cpy = y_val.astype('float')\n",
    "unique,counts = np.unique(y_val_cpy,return_counts=True)\n",
    "print(unique,counts)\n",
    "y_val_cpy[y_val_cpy==unique[0]]=1.0/(counts[0]+2)\n",
    "y_val_cpy[y_val_cpy==unique[1]]=(counts[1]+1.0)/(counts[1]+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(w,x,b):\n",
    "    return 1/(1+np.exp(-(w@x+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X,y,w,b,lamda,alpha,N):\n",
    "    w_new = (1-alpha*lamda/N)*w + alpha*X*(y-sigmoid(w,X.T,b))\n",
    "    b_new = b + alpha*(y-sigmoid(w,X.T,b))\n",
    "    return w_new,b_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y, batchSize):\n",
    "    # loop over our dataset `X` in mini-batches of size `batchSize`\n",
    "    for i in np.arange(0, X.shape[0], batchSize):\n",
    "        # yield a tuple of the current batched data and labels\n",
    "        yield (X[i:i + batchSize], y[i:i + batchSize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_loss(A,n):# your code\n",
    "    loss=0\n",
    "    for Y in A:\n",
    "        loss += Y[0]*math.log(Y[1])+(1-Y[0])*math.log(1-Y[1]) \n",
    "    loss = -loss/n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1,)"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros((1,))\n",
    "b = 0\n",
    "lamda  = 0.0001\n",
    "alpha = 0.0001\n",
    "N = len(fcv)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "iteration:1\nTraining Loss:0.6552991440625178\nTest Loss:0.6216799191875771\n===========================================================================\niteration:2\nTraining Loss:0.5899504093091643\nTest Loss:0.5645912433935767\n===========================================================================\niteration:3\nTraining Loss:0.5376450468017597\nTest Loss:0.5185955466686065\n===========================================================================\niteration:4\nTraining Loss:0.4953452949234486\nTest Loss:0.4811179312138139\n===========================================================================\niteration:5\nTraining Loss:0.4607177842269182\nTest Loss:0.4502053772357796\n===========================================================================\niteration:6\nTraining Loss:0.43201116620223556\nTest Loss:0.4243957071932323\n===========================================================================\niteration:7\nTraining Loss:0.40792005329857134\nTest Loss:0.40259569163253994\n===========================================================================\niteration:8\nTraining Loss:0.38746914857339987\nTest Loss:0.3839836238254411\n===========================================================================\niteration:9\nTraining Loss:0.3699243012691913\nTest Loss:0.3679364008743245\n===========================================================================\niteration:10\nTraining Loss:0.35472727885274635\nTest Loss:0.3539765039079248\n===========================================================================\niteration:11\nTraining Loss:0.34144882027064943\nTest Loss:0.3417338531083492\n===========================================================================\niteration:12\nTraining Loss:0.32975501183164174\nTest Loss:0.3309183847754427\n===========================================================================\niteration:13\nTraining Loss:0.31938314266484286\nTest Loss:0.3213002439334413\n===========================================================================\niteration:14\nTraining Loss:0.31012425112115755\nTest Loss:0.31269536737583653\n===========================================================================\niteration:15\nTraining Loss:0.30181039569982493\nTest Loss:0.3049548927351467\n===========================================================================\niteration:16\nTraining Loss:0.29430527895800895\nTest Loss:0.2979573002257887\n===========================================================================\niteration:17\nTraining Loss:0.2874972693182899\nTest Loss:0.29160252219300625\n===========================================================================\niteration:18\nTraining Loss:0.2812941533109972\nTest Loss:0.2858074827676511\n===========================================================================\niteration:19\nTraining Loss:0.2756191487461773\nTest Loss:0.28050268691278957\n===========================================================================\niteration:20\nTraining Loss:0.27040784585353755\nTest Loss:0.27562958702863327\n===========================================================================\niteration:21\nTraining Loss:0.26560583812342137\nTest Loss:0.27113853126412746\n===========================================================================\niteration:22\nTraining Loss:0.2611668707347844\nTest Loss:0.26698715111215393\n===========================================================================\niteration:23\nTraining Loss:0.25705138105688347\nTest Loss:0.26313908375226086\n===========================================================================\niteration:24\nTraining Loss:0.2532253388312813\nTest Loss:0.25956295170834337\n===========================================================================\niteration:25\nTraining Loss:0.24965931739417382\nTest Loss:0.2562315419516265\n===========================================================================\niteration:26\nTraining Loss:0.24632774449024541\nTest Loss:0.25312114082534004\n===========================================================================\niteration:27\nTraining Loss:0.2432082937828996\nTest Loss:0.2502109916324353\n===========================================================================\niteration:28\nTraining Loss:0.24028138741329827\nTest Loss:0.24748284948063623\n===========================================================================\niteration:29\nTraining Loss:0.23752978683073997\nTest Loss:0.24492061377030055\n===========================================================================\nFinal Weights:\n[0.90106319]\nFinal Intercept: [-0.1014738]\n"
    }
   ],
   "source": [
    "import math\n",
    "lossHistoryTrain = []\n",
    "lossHistoryTest = []\n",
    "epochs = range(1,30)\n",
    "for epoch in epochs:\n",
    "    # initialize the total loss for the epoch\n",
    "    epochLossTrain = []\n",
    "    epochLossTest = []\n",
    "    # loop over our data in batches\n",
    "    for (batchX, batchY) in next_batch(fcv, y_val_cpy, 1):\n",
    "        preds = sigmoid(w,batchX,b)\n",
    "        loss = -(batchY*math.log(preds)+(1-batchY)*math.log(1-preds))\n",
    "        epochLossTrain.append(loss)\n",
    "        w, b = update_weights(batchX,batchY,w,b,lamda,alpha,N)\n",
    "        \n",
    "    avgLossTrain = np.average(epochLossTrain)\n",
    "    lossHistoryTrain.append(avgLossTrain)\n",
    "    print(\"iteration:{}\".format(epoch))\n",
    "    print(\"Training Loss:{}\".format(avgLossTrain))\n",
    "    y_pred = [sigmoid(w,x.reshape(-1,1),b) for x in dec_fun(X_test)]\n",
    "    avgLossTest = compute_log_loss(zip(y_test,y_pred),len(y_test))\n",
    "    lossHistoryTest.append(avgLossTest)\n",
    "    print(\"Test Loss:{}\".format(avgLossTest))\n",
    "    print('='*75)\n",
    "print('Final Weights:')\n",
    "print(w)\n",
    "print('Final Intercept:',b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}