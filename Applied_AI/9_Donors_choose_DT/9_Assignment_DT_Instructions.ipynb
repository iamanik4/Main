{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKMeoHjWN-3k"
   },
   "source": [
    "# Assignment 8: DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACUkHex3N-3m"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_eassay (TFIDF)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_eassay (TFIDF W2V)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (best `depth` in range [1, 5, 10, 50], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>n_estimators</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n",
    "        <ul>\n",
    "            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n",
    "            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n",
    "            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n",
    "        </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "   <li><b>Task 2: </b>For this task consider set-1 features. Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n",
    "   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM), you need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n",
    "  Note: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n",
    "  </li>\n",
    "    <br>\n",
    "    <li>You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6FUMj5TN-3y"
   },
   "source": [
    "<h1>1. Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQmid3VAN-31"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqY4ES_3N-33"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data = pandas.read_csv(r'D:\\git\\preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "\n",
       "  clean_categories                 clean_subcategories  \\\n",
       "0     math_science  appliedsciences health_lifescience   \n",
       "1     specialneeds                        specialneeds   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lXoccSzN-37"
   },
   "source": [
    "<h2>1.2 Splitting data into Train and cross validation(or test): Stratified Sampling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSw1_vFcN-38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78931, 8) (78931,)\n",
      "(16388, 8) (78931,)\n",
      "(13929, 8) (13929,)\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop('project_is_approved',axis=1)\n",
    "y = data['project_is_approved']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.15,random_state=42,stratify=y)\n",
    "X_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,test_size=0.15,random_state=42,stratify=y_train)\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_train.shape)\n",
    "print(X_cv.shape,y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNXtHNGVN-4B"
   },
   "source": [
    "<h2>1.3 Make Data Model Ready: encoding eassay, and project_title</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'D:\\git\\glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def tfidf_w2v(vectorizer, data):\n",
    "    dictionary = dict(zip(vectorizer.get_feature_names(), list(vectorizer.idf_)))\n",
    "    tfidf_words = set(vectorizer.get_feature_names())\n",
    "    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "    for sentence in tqdm(data): # for each review/sentence\n",
    "        vector = np.zeros(300) # as word vectors are of zero length\n",
    "        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "        for word in sentence.split(): # for each word in a review/sentence\n",
    "            if (word in glove_words) and (word in tfidf_words):\n",
    "                vec = model[word] # getting the vector for each word\n",
    "                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "                tf_idf_weight += tf_idf\n",
    "        if tf_idf_weight != 0:\n",
    "            vector /= tf_idf_weight\n",
    "        tfidf_w2v_vectors.append(vector)\n",
    "\n",
    "    return np.array(tfidf_w2v_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEevUQOcN-4C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 78931/78931 [07:33<00:00, 174.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 16388/16388 [01:33<00:00, 176.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 13929/13929 [01:19<00:00, 175.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Vectorization\n",
      "==================================================\n",
      "(78931, 5000) (78931,)\n",
      "(16388, 5000) (16388,)\n",
      "(13929, 5000) (13929,)\n",
      "==================================================\n",
      "(78931, 300) (78931,)\n",
      "(16388, 300) (16388,)\n",
      "(13929, 300) (13929,)\n",
      "==================================================\n",
      "['000', '10', '100', '100 free', '100 percent', '100 students', '100 students receive', '100 students receive free', '11', '12']\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "TfidfVec = TfidfVectorizer(ngram_range=(1,4),min_df=10,max_features=5000)\n",
    "TfidfVec.fit(X_train['essay'].values)\n",
    "\n",
    "X_train_TfidfVec = TfidfVec.transform(X_train['essay'].values)\n",
    "X_train_TfidfW2V = tfidf_w2v(TfidfVec, X_train['essay'].values)\n",
    "X_test_TfidfVec = TfidfVec.transform(X_test['essay'].values)\n",
    "X_test_TfidfW2V = tfidf_w2v(TfidfVec, X_test['essay'].values)\n",
    "X_cv_TfidfVec = TfidfVec.transform(X_cv['essay'].values)\n",
    "X_cv_TfidfW2V = tfidf_w2v(TfidfVec, X_cv['essay'].values)\n",
    "\n",
    "print('After Vectorization')\n",
    "print('='*50)\n",
    "print(X_train_TfidfVec.shape, y_train.shape)\n",
    "print(X_test_TfidfVec.shape, y_test.shape)\n",
    "print(X_cv_TfidfVec.shape, y_cv.shape)\n",
    "print('='*50)\n",
    "print(X_train_TfidfW2V.shape, y_train.shape)\n",
    "print(X_test_TfidfW2V.shape, y_test.shape)\n",
    "print(X_cv_TfidfW2V.shape, y_cv.shape)\n",
    "print('='*50)\n",
    "print(TfidfVec.get_feature_names()[:10])\n",
    "feature_names.extend(TfidfVec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9B9fYus1N-4I"
   },
   "source": [
    "<h2>1.4 Make Data Model Ready: encoding numerical, categorical features</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-yI_TVhN-4J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ak', 'al', 'ar', 'az', 'ca']\n",
      "['dr', 'mr', 'mrs', 'ms', 'teacher']\n",
      "['grades_3_5', 'grades_6_8', 'grades_9_12', 'grades_prek_2']\n",
      "['appliedlearning', 'care_hunger', 'health_sports', 'history_civics', 'literacy_language']\n",
      "['appliedsciences', 'care_hunger', 'charactereducation', 'civics_government', 'college_careerprep']\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding \n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# make sure you featurize train and test data separatly\n",
    "\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "    \n",
    "# encoding categorical features using tfidf vectorizer\n",
    "    \n",
    "# encoding school_state\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['school_state'].values)\n",
    "\n",
    "X_train_state_tfidf = vectorizer.transform(X_train['school_state'].values)\n",
    "X_test_state_tfidf = vectorizer.transform(X_test['school_state'].values)\n",
    "X_cv_state_tfidf = vectorizer.transform(X_cv['school_state'].values)\n",
    "\n",
    "\n",
    "print(vectorizer.get_feature_names()[:5])\n",
    "feature_names.extend(vectorizer.get_feature_names())\n",
    "\n",
    "# encoding teacher_prefix\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['teacher_prefix'].values)\n",
    "\n",
    "X_train_teacher_prefix_tfidf = vectorizer.transform(X_train['teacher_prefix'].values)\n",
    "X_test_teacher_prefix_tfidf = vectorizer.transform(X_test['teacher_prefix'].values)\n",
    "X_cv_teacher_prefix_tfidf = vectorizer.transform(X_cv['teacher_prefix'].values)\n",
    "\n",
    "print(vectorizer.get_feature_names()[:5])\n",
    "feature_names.extend(vectorizer.get_feature_names())\n",
    "\n",
    "# encoding project_grade_category\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['project_grade_category'].values)\n",
    "\n",
    "X_train_project_grade_category_tfidf = vectorizer.transform(X_train['project_grade_category'].values)\n",
    "X_test_project_grade_category_tfidf = vectorizer.transform(X_test['project_grade_category'].values)\n",
    "X_cv_project_grade_category_tfidf = vectorizer.transform(X_cv['project_grade_category'].values)\n",
    "\n",
    "print(vectorizer.get_feature_names()[:5])\n",
    "feature_names.extend(vectorizer.get_feature_names())\n",
    "\n",
    "# encoding clean_categories\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['clean_categories'].values)\n",
    "\n",
    "X_train_clean_categories_tfidf = vectorizer.transform(X_train['clean_categories'].values)\n",
    "X_test_clean_categories_tfidf = vectorizer.transform(X_test['clean_categories'].values)\n",
    "X_cv_clean_categories_tfidf = vectorizer.transform(X_cv['clean_categories'].values)\n",
    "\n",
    "print(vectorizer.get_feature_names()[:5])\n",
    "feature_names.extend(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# encoding clean_subcategories\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(X_train['clean_subcategories'].values)\n",
    "\n",
    "X_train_clean_subcategories_tfidf = vectorizer.transform(X_train['clean_subcategories'].values)\n",
    "X_test_clean_subcategories_tfidf = vectorizer.transform(X_test['clean_subcategories'].values)\n",
    "X_cv_clean_subcategories_tfidf = vectorizer.transform(X_cv['clean_subcategories'].values)\n",
    "\n",
    "print(vectorizer.get_feature_names()[:5])\n",
    "feature_names.extend(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00279308]\n",
      " [0.00579594]\n",
      " [0.00704402]\n",
      " [0.01440459]\n",
      " [0.00138981]\n",
      " [0.00613287]\n",
      " [0.00139355]\n",
      " [0.00130866]\n",
      " [0.00570264]\n",
      " [0.0009347 ]]\n",
      "=========================\n",
      "[[0.        ]\n",
      " [0.00155586]\n",
      " [0.00682187]\n",
      " [0.00059841]\n",
      " [0.00155586]\n",
      " [0.02752684]\n",
      " [0.        ]\n",
      " [0.00071809]\n",
      " [0.00251332]\n",
      " [0.00167555]]\n"
     ]
    }
   ],
   "source": [
    "# encoding numerical features\n",
    "# encoding price\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(X_train['price'].values.reshape(1,-1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(1,-1)).reshape(-1,1)\n",
    "X_cv_price_norm = normalizer.transform(X_cv['price'].values.reshape(1,-1)).reshape(-1,1)\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(1,-1)).reshape(-1,1)\n",
    "\n",
    "print(X_cv_price_norm[:10])\n",
    "\n",
    "# encoding teacher_number_of_previously_posted_projects\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "X_train_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)).reshape(-1,1)\n",
    "X_cv_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)).reshape(-1,1)\n",
    "X_test_teacher_number_of_previously_posted_projects_norm = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)).reshape(-1,1)\n",
    "\n",
    "print('='*25)\n",
    "print(X_train_teacher_number_of_previously_posted_projects_norm[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oFZ2BOVKN-4N"
   },
   "source": [
    "<h2>1.5 Appling  Decision Tree on different kind of featurization as mentioned in the instructions</h2>\n",
    "\n",
    "<br>Apply  Decision Tree on different kind of featurization as mentioned in the instructions\n",
    "<br> For Every model that you work on make sure you do the step 2 and step 3 of instrucations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag95KYbvN-4O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78931, 51)\n",
      "Final Data matrix\n",
      "==================================================\n",
      "(78931, 5101) (78931,)\n",
      "(13929, 5101) (13929,)\n",
      "(16388, 5101) (16388,)\n",
      "==================================================\n",
      "(78931, 401) (78931,)\n",
      "(13929, 401) (13929,)\n",
      "(16388, 401) (16388,)\n"
     ]
    }
   ],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "# concatenating all features\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_tr_tfidf = hstack((X_train_TfidfVec,X_train_state_tfidf,X_train_teacher_prefix_tfidf,X_train_project_grade_category_tfidf,X_train_clean_categories_tfidf,X_train_clean_subcategories_tfidf,X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "X_cv_tfidf = hstack((X_cv_TfidfVec,X_cv_state_tfidf,X_cv_teacher_prefix_tfidf,X_cv_project_grade_category_tfidf,X_cv_clean_categories_tfidf,X_cv_clean_subcategories_tfidf,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "X_te_tfidf = hstack((X_test_TfidfVec,X_test_state_tfidf,X_test_teacher_prefix_tfidf,X_test_project_grade_category_tfidf,X_test_clean_categories_tfidf,X_test_clean_subcategories_tfidf,X_test_price_norm,X_test_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "print(X_train_state_tfidf.shape)\n",
    "X_tr_tfidfw2v = hstack((X_train_TfidfW2V,X_train_state_tfidf,X_train_teacher_prefix_tfidf,X_train_project_grade_category_tfidf,X_train_clean_categories_tfidf,X_train_clean_subcategories_tfidf,X_train_price_norm,X_train_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "X_cv_tfidfw2v = hstack((X_cv_TfidfW2V,X_cv_state_tfidf,X_cv_teacher_prefix_tfidf,X_cv_project_grade_category_tfidf,X_cv_clean_categories_tfidf,X_cv_clean_subcategories_tfidf,X_cv_price_norm,X_cv_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "X_te_tfidfw2v = hstack((X_test_TfidfW2V,X_test_state_tfidf,X_test_teacher_prefix_tfidf,X_test_project_grade_category_tfidf,X_test_clean_categories_tfidf,X_test_clean_subcategories_tfidf,X_test_price_norm,X_test_teacher_number_of_previously_posted_projects_norm)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print('='*50)\n",
    "print(X_tr_tfidf.shape, y_train.shape)\n",
    "print(X_cv_tfidf.shape, y_cv.shape)\n",
    "print(X_te_tfidf.shape, y_test.shape)\n",
    "print('='*50)\n",
    "print(X_tr_tfidfw2v.shape, y_train.shape)\n",
    "print(X_cv_tfidfw2v.shape, y_cv.shape)\n",
    "print(X_te_tfidfw2v.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "def batch_predict(clf, data):\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs\n",
    "\n",
    "    y_data_pred = []\n",
    "    tr_loop = data.shape[0] - data.shape[0]%1000\n",
    "    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n",
    "    # in this for loop we will iterate unti the last 1000 multiplier\n",
    "    for i in range(0, tr_loop, 1000):\n",
    "        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n",
    "    # we will be predicting for the last data points\n",
    "    if data.shape[0]%1000 !=0:\n",
    "        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n",
    "    \n",
    "    return y_data_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#parameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\n",
    "max_depth=[1, 5, 10, 50]\n",
    "min_samples_split=[5, 10, 100, 500]\n",
    "train_auc = [[0 for samples in min_samples_split]for depth in max_depth]\n",
    "cv_auc = [[0 for samples in min_samples_split]for depth in max_depth]\n",
    "i=0\n",
    "for max_depth in depth:\n",
    "    j = 0\n",
    "    for samples in min_samples_split:\n",
    "        print(j)\n",
    "        clf_tfidf = DecisionTreeClassifier(max_depth = max_depth,min_samples_split=samples)\n",
    "        clf_tfidf.fit(X_tr_tfidf, y_train)\n",
    "\n",
    "        y_train_pred = batch_predict(clf_tfidf, X_tr_tfidf)    \n",
    "        y_cv_pred = batch_predict(clf_tfidf, X_cv_tfidf)\n",
    "        \n",
    "        train_auc[i][j] = roc_auc_score(y_train,y_train_pred)\n",
    "        cv_auc[i][j] = roc_auc_score(y_cv, y_cv_pred)\n",
    "        j += 1\n",
    "    i += 1\n",
    "    \n",
    "sns.heatmap(train_auc,annot=True,xticklabels=max_depth,yticklabels=min_samples_split)\n",
    "plt.title('Heatmap TFIDF vectorization (AUC vs Hyperparameters)')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Min Samples Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tfidf-w2v\n",
    "train_auc = [[0 for samples in min_samples_split]for depth in max_depth]\n",
    "cv_auc = [[0 for samples in min_samples_split]for depth in max_depth]\n",
    "i=j=0\n",
    "for max_depth in tqdm(depth):\n",
    "    for samples in min_samples_split:\n",
    "        clf_tfidfw2v = DecisionTreeClassifier(max_depth = max_depth,min_samples_split=samples)\n",
    "        clf_tfidfw2v.fit(X_tr_tfidfw2v, y_train)\n",
    "\n",
    "        y_train_pred = batch_predict(clf_tfidfw2v, X_tr_tfidfw2v)    \n",
    "        y_cv_pred = batch_predict(clf_tfidfw2v, X_cv_tfidfw2v)\n",
    "        \n",
    "        train_auc[i][j] = roc_auc_score(y_train,y_train_pred)\n",
    "        cv_auc[i][j] = roc_auc_score(y_cv, y_cv_pred)\n",
    "        j += 1\n",
    "    i += 1\n",
    "    \n",
    "sns.heatmap(train_auc,annot=True,xticklabels=max_depth,yticklabels=min_samples_split)\n",
    "plt.title('Heatmap TFIDF-W2V vectorization (AUC vs Hyperparameters)')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Min Samples Split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_auc = []\n",
    "cv_auc = []\n",
    "for max_depth in tqdm(depth):\n",
    "    clf_tfidfw2v = DecisionTreeClassifier(max_depth = max_depth)\n",
    "    clf_tfidfw2v.fit(X_tr_tfidfw2v, y_train)\n",
    "\n",
    "    y_train_pred = batch_predict(clf_tfidfw2v, X_tr_tfidfw2v)    \n",
    "    y_cv_pred = batch_predict(cclf_tfidfw2vlf, X_cv_tfidfw2v)\n",
    "\n",
    "    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n",
    "    # not the predicted outputs        \n",
    "    train_auc.append(roc_auc_score(y_train,y_train_pred))\n",
    "    cv_auc.append(roc_auc_score(y_cv, y_cv_pred))\n",
    "\n",
    "plt.plot(depth, train_auc, label='Train AUC')\n",
    "plt.plot(depth, cv_auc, label='CV AUC')\n",
    "\n",
    "plt.scatter(depth, train_auc, label='Train AUC points')\n",
    "plt.scatter(depth, cv_auc, label='CV AUC points')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Max_depth: hyperparameter\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"AUC Scores vs Max_depth\")\n",
    "plt.grid()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zqgP8b0N-4U"
   },
   "source": [
    "<h2>1.6 Getting top features using `feature_importances_`</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label\n",
    "#Top 20 features for Tfidf\n",
    "min_indices = np.argsort(clf_tfidf.feature_importances_)\n",
    "max_indices = min_indices[::-1][:20]\n",
    "np.take(feature_names,max_indices)[:20] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHvBlI_9N-4X"
   },
   "outputs": [],
   "source": [
    "'''#Top 20 features for Tfidf-w2v\n",
    "min_indices = np.argsort(clf_tfidfw2v.feature_importances_)\n",
    "max_indices = min_indices[::-1][:20]\n",
    "np.take(feature_names,max_indices)[:20] '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfyyOdUaN-4e"
   },
   "source": [
    "<h1>2. Summary</h1>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "9_Assignment_DT_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
